{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYqCXEc2vEjY"
   },
   "source": [
    "## Importing all required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OC5Eu5svEja"
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost, joblib, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer,StandardScaler\n",
    "in_dict=joblib.load(\"in_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaQzCjoovEjg"
   },
   "source": [
    "## Getting ready to read data CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zDAVYrEvEjh"
   },
   "outputs": [],
   "source": [
    "# csv files path\n",
    "train_path='train_users.csv'\n",
    "test_path='test_users.csv'\n",
    "sess_path='sessions.csv'\n",
    "\n",
    "# path dictionary\n",
    "Data_Path={\"train\":train_path,\"test\":test_path,\"session\":sess_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lb6frkg3vEjn"
   },
   "source": [
    "## Data Preprocessing and Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1gwugtzvEjp"
   },
   "outputs": [],
   "source": [
    "# Function for Data Preprocessing and feature extraction\n",
    "\n",
    "def Data_Preparation(Data_Path,Invoke_by_module=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------\n",
    "    Function perform Data Preprocessing and Feature Extraction\n",
    "        1.Read Csv Files   \n",
    "        2.Preprocess Data   \n",
    "        3.Extract Features  \n",
    "        4.Format Data      \n",
    "        5.Return Formated Data\n",
    "    -----------------------------------------------------------\n",
    "        Parameters\n",
    "        ----------\n",
    "        Data_Path <Dictionary>    : The Dict should contain Data and Absolute Path of all Data files.\n",
    "        Invoke_by_module <Boolean>: True if this Function is invoked from other file else False\n",
    "    \n",
    "        returns \n",
    "        --------\n",
    "        Final_DataFrame <DataFrame> : Contain Data that is Preprocessed with Extracted Features\n",
    "        Target_Labels <pd.series>   : contain Target variables as pandas series \n",
    "    -----------------------------------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if (Invoke_by_module):\n",
    "        print(\"Data_Preparation Module\")\n",
    "    else:\n",
    "        print(\"Invoking Function for Data Preparaion.....\")\n",
    "    \n",
    "    print(\"1.Reading Csv Files    ..\",end =\" \")\n",
    "    \n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # reading csv files\n",
    "    train_dataframe = Data_Path[\"train\"] \n",
    "    print(\"..\",end =\" \")\n",
    "    test_dataframe = pd.read_csv(Data_Path[\"test\"])\n",
    "    print(\"..\",end =\" \")\n",
    "    sessions_dataframe = pd.read_csv(Data_Path[\"session\"])\n",
    "    print(\">>> |Done| <1/5>\")\n",
    "    \n",
    "    print(\"2.Preprocessing Data   ..\",end =\" \")\n",
    "    \n",
    "    # storing target and ids \n",
    "    Target_Labels = train_dataframe['country_destination']\n",
    "    test_id = test_dataframe['id']\n",
    "    sessions_dataframe['id'] = sessions_dataframe['user_id']\n",
    "    print(\"..\",end =\" \")\n",
    "\n",
    "    # droping columns from dataframe\n",
    "    train_dataframe = train_dataframe.drop(['country_destination'], axis=1)\n",
    "    sessions_dataframe = sessions_dataframe.drop(['user_id'],axis=1) \n",
    "    print(\"..\",end =\" \")\n",
    "\n",
    "    # Pre-processing Session data \n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
    "    # Replacing all null value with \"NAN\"\n",
    "    sessions_dataframe.action = sessions_dataframe.action.fillna('NAN')\n",
    "    sessions_dataframe.action_type = sessions_dataframe.action_type.fillna('NAN')\n",
    "    sessions_dataframe.action_detail = sessions_dataframe.action_detail.fillna('NAN')\n",
    "    sessions_dataframe.device_type = sessions_dataframe.device_type.fillna('NAN')\n",
    "    print(\">>> |Done| <2/5>\")\n",
    "    \n",
    "    print(\"3.Extracting Features \",end =\" \")\n",
    "    \n",
    "    # Keeping Thresold value as 100 \n",
    "    action_threshold = 100 \n",
    "\n",
    "    # Any action count value less tha 100 is replaced by 'OTHERS'\n",
    "    actions = dict(zip(*np.unique(sessions_dataframe.action, return_counts=True)))\n",
    "    sessions_dataframe.action = sessions_dataframe.action.apply(lambda x: 'OTHER' if actions[x] < action_threshold else x)\n",
    "    \n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\n",
    "    # Obtaining unique value counts in order for features action:index 321\n",
    "    action_frequency = sessions_dataframe.action.value_counts().argsort()\n",
    "    action_detail_frequency = sessions_dataframe.action_detail.value_counts().argsort()\n",
    "    action_type_frequency = sessions_dataframe.action_type.value_counts().argsort()\n",
    "    device_type_frequency = sessions_dataframe.device_type.value_counts().argsort()\n",
    "    \n",
    "    # ref:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\n",
    "    # Grouping session data by 'id' \n",
    "    session_group = sessions_dataframe.groupby(['id'])\n",
    "\n",
    "    # basic inintialization\n",
    "    matrix = []\n",
    "    length = len(session_group)\n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # iterating through individual groups\n",
    "    for group in session_group:\n",
    "\n",
    "        # single group by id\n",
    "        group_set = group[1]\n",
    "\n",
    "        # feature extraction\n",
    "        features = []    \n",
    "        features.append(group[0]) # id\n",
    "        features.append(len(group_set)) # length of current group\n",
    "\n",
    "        # replacing all nan value by 0 in 'secs_elapsed' feature\n",
    "        secs = group_set.secs_elapsed.fillna(0).values   \n",
    "\n",
    "        # ref: https://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray-in-python\n",
    "        # ref: https://blog.csdn.net/Datawhale/article/details/80847662.\n",
    "        # Action feature value counts, no of unique actions, mean and std.  \n",
    "        action_count = [0] * len(action_frequency)\n",
    "        for i,v in enumerate(group_set.action.values):\n",
    "            action_count[action_frequency[v]] += 1\n",
    "        _, action_unique_count = np.unique(group_set.action.values, return_counts=True)\n",
    "        action_count += [len(action_unique_count), np.mean(action_unique_count), np.std(action_unique_count)]\n",
    "        features = features + action_count\n",
    "\n",
    "        # Action_detail feature value counts, no of unique Action_details, mean and std.     \n",
    "        action_detail_count = [0] * len(action_detail_frequency)\n",
    "        for i,v in enumerate(group_set.action_detail.values):\n",
    "            action_detail_count[action_detail_frequency[v]] += 1 \n",
    "        _, action_detail_unique_count = np.unique(group_set.action_detail.values, return_counts=True)\n",
    "        action_detail_count += [len(action_detail_unique_count), np.mean(action_detail_unique_count), np.std(action_detail_unique_count)]\n",
    "        features = features + action_detail_count\n",
    "\n",
    "        # Action_type feature value counts, no of unique Action_type, mean and std, log(sum of secs_elapsed) \n",
    "        action_type_secs = [0] * len(action_type_frequency)\n",
    "        action_type_count = [0] * len(action_type_frequency)\n",
    "        for i,v in enumerate(group_set.action_type.values):\n",
    "            action_type_secs[action_type_frequency[v]] += secs[i]   \n",
    "            action_type_count[action_type_frequency[v]] += 1  \n",
    "        action_type_secs = np.log(1 + np.array(action_type_secs)).tolist()\n",
    "        _, action_type_unique_count = np.unique(group_set.action_type.values, return_counts=True)\n",
    "        action_type_count += [len(action_type_unique_count), np.mean(action_type_unique_count), np.std(action_type_unique_count)]\n",
    "        features = features + action_type_count + action_type_secs    \n",
    "\n",
    "        # device_type feature value counts, no of unique device_type, mean and std.     \n",
    "        device_type_count  = [0] * len(device_type_frequency)\n",
    "        for i,v in enumerate(group_set.device_type .values):\n",
    "            device_type_count[device_type_frequency[v]] += 1 \n",
    "        device_type_count.append(len(np.unique(group_set.device_type.values)))\n",
    "        _, device_type_unique = np.unique(group_set.device_type.values, return_counts=True)\n",
    "        device_type_count += [len(device_type_unique), np.mean(device_type_unique), np.std(device_type_unique)]        \n",
    "        features = features + device_type_count    \n",
    "\n",
    "        # creating features from 'secs_elapsed' feature\n",
    "        secs_features = [0] * 5 \n",
    "        log_bin = [0] * 15\n",
    "\n",
    "        # stats features\n",
    "        if len(secs) > 0:\n",
    "            secs_features[0] = np.log(1 + np.sum(secs))\n",
    "            secs_features[1] = np.log(1 + np.mean(secs)) \n",
    "            secs_features[2] = np.log(1 + np.std(secs))\n",
    "            secs_features[3] = np.log(1 + np.median(secs))\n",
    "            secs_features[4] = secs_features[0] / float(features[1])\n",
    "\n",
    "        # bined features  ref: https://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html\n",
    "            secs_log = np.log(1 + secs).astype(int)\n",
    "            log_bin = np.bincount(minlength=15, x=secs_log).tolist()                      \n",
    "        features = features + secs_features + log_bin\n",
    "\n",
    "        # final feature matrix\n",
    "        matrix.append(features)\n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # creating feature names for matrix\n",
    "    feat_names = []    \n",
    "    for i in range(len(matrix[0])-1):\n",
    "        feat_names.append('feat_' + str(i)) \n",
    "\n",
    "    # converting feature matrix to array\n",
    "    matrix = np.array(matrix)\n",
    "    matrix_array = matrix[:, 1:].astype(np.float16)\n",
    "    matrix_id = matrix[:, 0]   \n",
    "\n",
    "    # creating dataframe from array matrix\n",
    "    session_matrix_dataframe = pd.DataFrame(matrix_array, columns=feat_names)\n",
    "    session_matrix_dataframe['id'] = matrix_id\n",
    "    session_matrix_dataframe.index = session_matrix_dataframe.id\n",
    "\n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    # Train and test dataframes concatination\n",
    "    dataframe_tt = pd.concat((train_dataframe, test_dataframe), axis=0, ignore_index=True)\n",
    "    dataframe_tt.index = dataframe_tt.id\n",
    "\n",
    "    # Pre-processing training data\n",
    "    dataframe_tt = dataframe_tt.fillna(-1)  \n",
    "    dataframe_tt = dataframe_tt.replace('-unknown-', -1) # replace all nan value with -1\n",
    "    dataframe_tt = dataframe_tt.drop(['date_first_booking'], axis=1)\n",
    "\n",
    "    # Feature extration from timestamp feature 'date_account_created'\n",
    "    dataframe_tt['n_null'] = np.array([sum(r == -1) for r in dataframe_tt.values]) # no of nan\n",
    "    date_acc_crt = np.vstack(dataframe_tt.date_account_created.astype(str).apply(lambda x: x.split('-')).values)\n",
    "    date_acc_crt=date_acc_crt.astype(int) \n",
    "    dataframe_tt['dac_year'] = date_acc_crt[:,0]  # date_account_created year\n",
    "    dataframe_tt['dac_month'] = date_acc_crt[:,1] # date_account_created month\n",
    "    dataframe_tt['dac_day'] = date_acc_crt[:,2]   # date_account_created day\n",
    "    \n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.isocalendar.html\n",
    "    acc_cret_dates = [datetime(x[0],x[1],x[2]) for x in date_acc_crt]\n",
    "    dataframe_tt['dac_week_number'] = np.array([d.isocalendar()[1] for d in acc_cret_dates]) # date_account_created week_number\n",
    "    dataframe_tt['dac_week_day'] = np.array([d.weekday() for d in acc_cret_dates]) # date_account_created week_day\n",
    "\n",
    "    # one-hot-encoding for week_day\n",
    "    dataFrame_tt_wd = pd.get_dummies(dataframe_tt.dac_week_day, prefix='dac_week_day')  \n",
    "    dataframe_tt = dataframe_tt.drop(['date_account_created', 'dac_week_day'], axis=1)\n",
    "    dataframe_tt = pd.concat((dataframe_tt, dataFrame_tt_wd), axis=1)\n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # function\n",
    "    def func(s):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function takes integer and convert it to datetime and return it.\n",
    "        Input type:  Integer\n",
    "        return type: Datetime\n",
    "        \"\"\"\n",
    "        \n",
    "        s=str(s)\n",
    "        return datetime(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8]),\n",
    "               hour=int(s[8:10]), minute=int(s[10:12]), second=int(s[12:]))\n",
    "\n",
    "    # Feature extration from feature 'timestamp_first_active'\n",
    "    dataframe_tt['timestamp_first_active'] = pd.to_datetime(dataframe_tt.timestamp_first_active.apply(func))\n",
    "    first_acc_dates= list(dataframe_tt['timestamp_first_active'])\n",
    "    dataframe_tt['tfa_day'] = dataframe_tt.timestamp_first_active.dt.day     # first_active day\n",
    "    dataframe_tt['tfa_month'] = dataframe_tt.timestamp_first_active.dt.month # first_active month\n",
    "    dataframe_tt['tfa_year'] = dataframe_tt.timestamp_first_active.dt.year   # first_active year\n",
    "    dataframe_tt['tfa_hour'] = dataframe_tt.timestamp_first_active.dt.hour   # first_active hour\n",
    "    dataframe_tt['tfa_week_number'] = np.array([d.isocalendar()[1] for d in first_acc_dates]) # first_active week_number\n",
    "    dataframe_tt['tfa_week_day'] = np.array([d.weekday() for d in first_acc_dates]) # first_active week_day\n",
    "\n",
    "    # one-hot-encoding for week_day \n",
    "    dataFrame_tt_wd = pd.get_dummies(dataframe_tt.tfa_week_day, prefix='tfa_week_day') \n",
    "    dataframe_tt = dataframe_tt.drop(['timestamp_first_active', 'tfa_week_day'], axis=1)\n",
    "    dataframe_tt = pd.concat((dataframe_tt, dataFrame_tt_wd), axis=1)\n",
    "    \n",
    "    # ref: https://drive.google.com/file/d/1NIq_IuTgmPEWUKYBv_nfU4rwIUoykYLE/view?usp=sharing\n",
    "    # Extracting difference sign and difference in second between the time account_created and first_active\n",
    "    dataframe_tt['dac_tfa_secs'] = np.array([np.log(1+abs((acc_cret_dates[i]-first_acc_dates[i]).total_seconds())) for i in range(len(acc_cret_dates))])\n",
    "    dataframe_tt['sig_dac_tfa'] = np.array([np.sign((acc_cret_dates[i]-first_acc_dates[i]).total_seconds()) for i in range(len(acc_cret_dates))])\n",
    "    print(\">>> |Done| <3/5>\")\n",
    "    \n",
    "    # function indicator_season\n",
    "    def indicator_season(date_for_season):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function takes Datetime type and return integer \n",
    "        Compute season for a given date\n",
    "        Input type:  datatime type\n",
    "        return type: Integer\n",
    "        \"\"\"\n",
    "        \n",
    "        date_for_season=date_for_season.date().replace(year=2000)\n",
    "\n",
    "        winter=[date(2000,  1,  1),date(2000,  3, 20),\n",
    "                date(2000, 12, 21),date(2000, 12, 31)]    \n",
    "        spring=[date(2000,  3, 21),  date(2000,  6, 20)]  \n",
    "        summer=[date(2000,  6, 21),  date(2000,  9, 22)]\n",
    "        autumn=[date(2000,  9, 23),  date(2000, 12, 20)]  \n",
    "\n",
    "        if (winter[0]<=date_for_season<=winter[1]) or (winter[2]<=date_for_season<=winter[3]):\n",
    "            sesn=0\n",
    "        elif spring[0]<=date_for_season<=spring[1]:\n",
    "            sesn=1\n",
    "        elif summer[0]<=date_for_season<=summer[1]:\n",
    "            sesn=2\n",
    "        elif autumn[0]<=date_for_season<=autumn[1]:\n",
    "            sesn=3\n",
    "        return sesn\n",
    "\n",
    "    print(\"4.Formatting Data      ..\",end =\" \")\n",
    "    \n",
    "    # Extracting Season feature from account_created and first_active dates\n",
    "    dataframe_tt['season_dac'] = np.array([indicator_season(date_for_season) for date_for_season in acc_cret_dates])\n",
    "    dataframe_tt['season_tfa'] = np.array([indicator_season(date_for_season) for date_for_season in first_acc_dates])\n",
    "\n",
    "    # ref: https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\n",
    "    # Pre-processing 'age' feature\n",
    "    age_value = dataframe_tt.age.values\n",
    "    age_value = np.where((age_value<2000)&(age_value>1900), 2014-age_value, age_value) # replace all dob value to correct age\n",
    "    age_value = np.where((age_value<14)&(age_value>0), 4, age_value) # replace all age value <14 by value 4 \n",
    "    age_value = np.where((age_value<2016)&(age_value>2010), 9, age_value) # replace all 2016<age_value>2010 by value 9 \n",
    "    age_value = np.where(age_value>99, 110, age_value) # replace all age_value>99 by value 110\n",
    "    dataframe_tt['age'] = age_value\n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # ref: https://drive.google.com/file/d/1NIq_IuTgmPEWUKYBv_nfU4rwIUoykYLE/view\n",
    "    # Age-binning into 20 equal intervals with an interval_value=5 \n",
    "    age_interval =[i for i in range(0,101,5)]\n",
    "    def get_interv_value(age):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function take an integer and return its coressponding interval that it belong to.\n",
    "        input type:  integer(age)\n",
    "        return type: integer(interval_value)\n",
    "        \"\"\"\n",
    "        \n",
    "        interval_value = 20\n",
    "        for i in range(len(age_interval)):\n",
    "            if age < age_interval[i]:\n",
    "                interval_value = i \n",
    "                break\n",
    "        return interval_value\n",
    "    dataframe_tt['age_interv'] = dataframe_tt.age.apply(lambda x: get_interv_value(x))\n",
    "\n",
    "    # one-hot-encoding binned age features\n",
    "    dataFrame_age_interval = pd.get_dummies(dataframe_tt.age_interv, prefix='age_interv')\n",
    "    dataframe_tt = dataframe_tt.drop(['age_interv'], axis=1)\n",
    "    dataframe_tt = pd.concat((dataframe_tt, dataFrame_age_interval), axis=1)\n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "    # Creating dummy variables (one-hot-encoding) for train data features\n",
    "    one_hot_features = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', \n",
    "                        'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "    for feature in one_hot_features:\n",
    "        dataFrame_tt_dummy = pd.get_dummies(dataframe_tt[feature], prefix=feature)\n",
    "        dataframe_tt = dataframe_tt.drop([feature], axis=1)\n",
    "        dataframe_tt = pd.concat((dataframe_tt, dataFrame_tt_dummy), axis=1)    \n",
    "    dataframe_tt.reset_index(drop=True,inplace=True)\n",
    "    session_matrix_dataframe.reset_index(drop=True,inplace=True)\n",
    "    print(\">>> |Done| <4/5>\")\n",
    "    \n",
    "    print(\"5.Returning Final Data ..\",end =\" \")\n",
    "    \n",
    "    # Merging pre-processed session data and train data on 'id' to obtain final dataframe\n",
    "    Final_DataFrame = pd.merge(dataframe_tt, session_matrix_dataframe, on=\"id\",how='left')\n",
    "    print(\"..\",end =\" \")\n",
    "\n",
    "    # replace all nan by -2\n",
    "    Final_DataFrame = Final_DataFrame.fillna(-2) \n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # Nan count for each row\n",
    "    Final_DataFrame['all_null'] = np.array([sum(r<0) for r in Final_DataFrame.drop(['id'], axis=1).values]) \n",
    "    print(\">>>\",end =\" \")\n",
    "\n",
    "    # formating data to avoid data_leakage\n",
    "    tr_cols, ts_cols=in_dict[\"features\"][\"fin_cols\"], list(Final_DataFrame.columns)\n",
    "    Final_Formated=pd.DataFrame(data=np.zeros((Final_DataFrame.shape[0],len(tr_cols))),columns=tr_cols)\n",
    "\n",
    "    for i in ts_cols:\n",
    "        Final_Formated[i]=Final_DataFrame[i]\n",
    "    \n",
    "    print(\"|Done| <5/5>\")\n",
    "    \n",
    "    # returning prepared data\n",
    "    return Final_Formated, Target_Labels ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-eadLD-vEjw"
   },
   "source": [
    "## Feature Selection by using Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DXgnzcPvEjy"
   },
   "outputs": [],
   "source": [
    "def Feature_Selection(Final_DataFrame, Target_Labels, Keep_percent=0.7, kag_sub=False, Rerun=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------------------------------------\n",
    "    Perform Feature Selection with Data for preserving specified percent of important features \n",
    "        1.Performing Train Validation split on Data \n",
    "        2.Training Xgboost for Feature Importance\n",
    "        3.Format Data by using Importance Score\n",
    "        4.Returning Data and Selected Features               \n",
    "    ------------------------------------------------------------------------------------------\n",
    "        Parameters\n",
    "        ----------\n",
    "        Final_DataFrame <DataFrame> : Data that is Preprocessed and Formated\n",
    "        Target_Labels <pd.series>   : Target variables\n",
    "        Keep_percent <float>        : (defaulf=0.7) \n",
    "                                      value between 0-1 which specify percent of \n",
    "                                      Features to preserve\n",
    "        kag_sub <Boolean>           : (defaulf=False) \n",
    "                                       True : Feature_Selection for kaggle submission \n",
    "                                       False: Normal Feature_Selection \n",
    "        Rerun <Boolean>             : (defaulf=False)\n",
    "                                       True : Rerun Model for Feature selection \n",
    "                                       False: Load saved Model for Feature_Selection                                 \n",
    "                                       \n",
    "        returns \n",
    "        --------\n",
    "        Data <numpy array>          : Data with only Selected Features\n",
    "        Imp_Features <list>         : Mask of Selected Features \n",
    "    -------------------------------------------------------------------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if kag_sub:print(\"Feature Selection\")\n",
    "    print(\"1.Performing Train Validation split on Data ..\",end =\" \")\n",
    "    \n",
    "    # Data inintialization\n",
    "    len_tr = len(Target_Labels)\n",
    "    data = Final_DataFrame.values[:len_tr]\n",
    "    Test_x = Final_DataFrame.values[len_tr:]\n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # lable encoding\n",
    "    label_en = LabelEncoder()\n",
    "    labels = label_en.fit_transform(Target_Labels.values)\n",
    "    print(\"..\",end =\" \")\n",
    "\n",
    "    # train val split\n",
    "    # ref: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    random_state=0\n",
    "    Train_x, CV_x, Train_y, CV_y = train_test_split(data, labels, test_size=0.2, random_state=0,stratify=labels)\n",
    "    test_id, Train_x, CV_x, Test_x=Test_x[:,0:1], Train_x[:,1:], CV_x[:,1:], Test_x[:,1:]\n",
    "    data=data[:,1:]\n",
    "    print(\">>> |Done| <1/3>\")\n",
    "    \n",
    "    print(\"2.Training Xgboost for Feature Importance   .. ..\",end =\" \")\n",
    "    \n",
    "    # ref: https://xgboost.readthedocs.io/en/latest/parameter.htm\n",
    "    # Feature selection by Xg-boost\n",
    "    if Rerun==True:\n",
    "        \n",
    "        params = {'eta': 0.09,'max_depthresold': 6,'subsample': 0.5,'colsample_bytree': 0.5,'objective': 'multi:softprob',\n",
    "                              'eval_metric': 'mlogloss','num_class': 12,\"n_jobs\":6, \"silent\": 1}\n",
    "        num_rounds = 900\n",
    "        xg_tr = xgboost.DMatrix(Train_x, label=Train_y)  \n",
    "        xg_cv = xgboost.DMatrix(CV_x, label=CV_y)  \n",
    "        watchlist = [(xg_tr,'TRAIN'), (xg_cv, 'CV')]\n",
    "        xgb = xgboost.train(params, xg_tr, num_rounds, watchlist, early_stopping_rounds=10,verbose_eval=0)\n",
    "        \n",
    "        print(\"..\",end =\" \")\n",
    "    \n",
    "    # load saved model\n",
    "    else:\n",
    "        \n",
    "        xgb=in_dict[\"Best_Model\"][\"Feature_Sel_Model\"]\n",
    "        print(\"..\",end =\" \")\n",
    "        \n",
    "    print(\">>> |Done| <2/3>\")\n",
    "    \n",
    "    \n",
    "    print(\"3.Returning Selected Features               .. ..\",end =\" \")\n",
    "    \n",
    "    # getting feature importance for each\n",
    "    imp_scores = xgb.get_fscore()\n",
    "\n",
    "    # mapping important feature value with features\n",
    "    Imp_Features = np.zeros(Train_x.shape[1])\n",
    "    for k,v in imp_scores.items():\n",
    "        Imp_Features[int(k[1:])] = v\n",
    "\n",
    "    # normalization of feature importance    \n",
    "    Imp_Features = Imp_Features/float(np.max(Imp_Features))\n",
    "\n",
    "    # finding thresold to select important feature\n",
    "    score=Imp_Features\n",
    "    thresold = np.sort(score)[::-1][int(len(score)*Keep_percent)] # selecting top 70% fetaures\n",
    "    Imp_Features = score > thresold\n",
    "    print(\"..\",end =\" \")\n",
    "    \n",
    "    # Re-initilaizing data to keep only important feature\n",
    "    data = data[:, Imp_Features]\n",
    "    Train_x,CV_x,Test_x = Train_x[:, Imp_Features],CV_x[:, Imp_Features],Test_x[:, Imp_Features]\n",
    "    print(\">>> |Done| <3/3>\")\n",
    "\n",
    "    print('Selected %s Important Features from %s Features' %(Train_x.shape[1], Imp_Features.shape[0]))\n",
    "    \n",
    "    # return data after feature selection\n",
    "    if kag_sub==True:\n",
    "        \n",
    "        data = [Train_x, CV_x, Train_y, CV_y, Test_x, test_id, label_en]\n",
    "    \n",
    "    \n",
    "    return data, Imp_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBGUuga6vEj8"
   },
   "outputs": [],
   "source": [
    "def Get_Imp_Features(Model, Top, Want_to_load=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------------------------------------\n",
    "    Get Important Features names and plot Barh_plot with their score\n",
    "        1.Load Model and get Feature_Importances\n",
    "        2.Extract Top Features using Feature_Importances\n",
    "        3.Plot a Barh_plot to see relative Imporatance\n",
    "    -----------------------------------------------------------------------------------------\n",
    "        Parameters\n",
    "        ----------\n",
    "        Model <str> or <ML Model>  : str or ML-Model type \n",
    "        Top <int>                  : Top Important Features names to return\n",
    "        Want_to_load <Boolean>     : (defaulf=False)\n",
    "                                      True : When Model has to be loaded from specified path  \n",
    "                                      False: When ML-Model is direcly passed \n",
    "        \n",
    "        returns \n",
    "        --------        \n",
    "        Top_Features <list>        : Feature_names of Top features in order\n",
    "    \n",
    "    -----------------------------------------------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # load Ml Model for feature_importances_ \n",
    "    if (Want_to_load):\n",
    "        xgb=in_dict[\"Best_Model\"][Model]    \n",
    "    \n",
    "    else:\n",
    "        xgb=Model       \n",
    "\n",
    "    # get feature_importances and extract Top features      \n",
    "    f_index_sort=xgb.feature_importances_.argsort()[::-1]\n",
    "    Top_Features=np.take(in_dict[\"features\"][\"selected_features_names\"],f_index_sort)[0:Top]\n",
    "    Top_scores=np.take(xgb.feature_importances_,f_index_sort)[0:Top]\n",
    "\n",
    "    # ref: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "    # plot barh_plot for relative importance\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(Top_Features)),Top_scores[::-1], color=\"green\",label=\"Feature Importance\")\n",
    "    plt.yticks(range(len(Top_Features)), Top_Features[::-1])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.legend(loc=0)\n",
    "    plt.show()\n",
    "    \n",
    "    return Top_Features[0:Top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBbF-GWjvEkD"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get_Imp_Features(Model, Top, Want_to_load=False):\n",
    "Feature_Selection(Final_DataFrame, Target_Labels, Keep_percent=0.7, kag_sub=False, Rerun=True):\n",
    "Data_Preparation(Data_Path,Invoke_by_module=False):\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Data_Preparation_Airbnb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
