{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Egxl5movK-c"
   },
   "source": [
    "# Function Definition of FUNCTION1() and FUNCTION2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpHw5ZgIvK-d"
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import joblib, xgboost, warnings \n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# https://joblib.readthedocs.io/en/latest/\n",
    "# get sess_csv and saved_data\n",
    "sess_csv=pd.read_csv('sessions.csv')\n",
    "in_dict=joblib.load(\"in_dict\")\n",
    "\n",
    "# FUNCTION-1 DEFINITION\n",
    "def FUNCTION1(data, prob_label=\"label\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------\n",
    "    Function returns predictions for a given raw input\n",
    "        1.Read, Preprocess and Extract data for given Test_set \n",
    "        2.Load pretrained model and preform predictions\n",
    "        3.Return formated predictions\n",
    "    -----------------------------------------------------------\n",
    "        Parameters\n",
    "        ----------\n",
    "        Data <List of Data_Frame>   : Contain Test_set Data\n",
    "        prob_label <string>         : Indicator variable for type of prediction \n",
    "                                      (\"probability\" or \"label\")\n",
    "    \n",
    "        returns \n",
    "        --------\n",
    "        pred_targets <ndarray>      : contain Target labels or Probabilites\n",
    "        \n",
    "    -----------------------------------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # load  test samples\n",
    "    test_dataframe=data[0]\n",
    "    s_id=test_dataframe[\"id\"]\n",
    "    \n",
    "    # if session samples exists\n",
    "    if (isinstance(data[1], pd.DataFrame)):\n",
    "        \n",
    "        # load session samples\n",
    "        sessions_dataframe=data[1]\n",
    "        sessions_dataframe['id'] = sessions_dataframe['user_id']\n",
    "        sessions_dataframe = sessions_dataframe.drop(['user_id'],axis=1) \n",
    "\n",
    "        # Pre-processing Session data\n",
    "        # Replacing all null value with \"NAN\"\n",
    "        sessions_dataframe.action = sessions_dataframe.action.fillna('NAN')\n",
    "        sessions_dataframe.action_type = sessions_dataframe.action_type.fillna('NAN')\n",
    "        sessions_dataframe.action_detail = sessions_dataframe.action_detail.fillna('NAN')\n",
    "        sessions_dataframe.device_type = sessions_dataframe.device_type.fillna('NAN')\n",
    "        \n",
    "        # Keeping Thresold value as 100 and load exact feature representation \n",
    "        action_threshold = 100 \n",
    "        actions=in_dict[\"actions\"]\n",
    "        sessions_dataframe.action = sessions_dataframe.action.apply(lambda x: 'OTHER' if actions[x] < action_threshold else x)\n",
    "        action_frequency =in_dict[\"frequency\"][0]\n",
    "        action_detail_frequency = in_dict[\"frequency\"][1]\n",
    "        action_type_frequency = in_dict[\"frequency\"][2]\n",
    "        device_type_frequency = in_dict[\"frequency\"][3]\n",
    "\n",
    "    \n",
    "    # if session data don't exists\n",
    "    if (isinstance(data[1], pd.DataFrame))==False:\n",
    "        matrix=np.concatenate((np.array(s_id).reshape(s_id.shape[0],1),\\\n",
    "                           np.full((s_id.shape[0],457),-2)), axis=1)\n",
    "    else:\n",
    "        # Grouping session data by 'id'\n",
    "        session_group = sessions_dataframe.groupby(['id'])\n",
    "\n",
    "        # basic inintialization\n",
    "        matrix = []\n",
    "        length = len(session_group)\n",
    "        \n",
    "        # iterating through individual groups\n",
    "        for group in session_group:\n",
    "            group_set = group[1]\n",
    "            \n",
    "            # feature extraction\n",
    "            features = []\n",
    "            features.append(group[0])\n",
    "            features.append(len(group_set))\n",
    "            secs = group_set.secs_elapsed.fillna(0).values   \n",
    "            \n",
    "            # Action feature value counts, no of unique actions, mean and std. \n",
    "            action_count = [0] * len(action_frequency)\n",
    "            for i,v in enumerate(group_set.action.values):\n",
    "                action_count[action_frequency[v]] += 1\n",
    "            _, action_unique_count = np.unique(group_set.action.values, return_counts=True)\n",
    "            action_count += [len(action_unique_count), np.mean(action_unique_count), np.std(action_unique_count)]\n",
    "            features = features + action_count\n",
    "            \n",
    "            # Action_detail feature value counts, no of unique Action_details, mean and std.  \n",
    "            action_detail_count = [0] * len(action_detail_frequency)\n",
    "            for i,v in enumerate(group_set.action_detail.values):\n",
    "                action_detail_count[action_detail_frequency[v]] += 1 \n",
    "            _, action_detail_unique_count = np.unique(group_set.action_detail.values, return_counts=True)\n",
    "            action_detail_count += [len(action_detail_unique_count), np.mean(action_detail_unique_count), np.std(action_detail_unique_count)]\n",
    "            features = features + action_detail_count\n",
    "            \n",
    "            # Action_type feature value counts, no of unique Action_type, mean and std, log(sum of secs_elapsed) \n",
    "            action_type_secs = [0] * len(action_type_frequency)\n",
    "            action_type_count = [0] * len(action_type_frequency)\n",
    "            for i,v in enumerate(group_set.action_type.values):\n",
    "                action_type_secs[action_type_frequency[v]] += secs[i]   \n",
    "                action_type_count[action_type_frequency[v]] += 1  \n",
    "            action_type_secs = np.log(1 + np.array(action_type_secs)).tolist()\n",
    "            _, action_type_unique_count = np.unique(group_set.action_type.values, return_counts=True)\n",
    "            action_type_count += [len(action_type_unique_count), np.mean(action_type_unique_count), np.std(action_type_unique_count)]\n",
    "            features = features + action_type_count + action_type_secs    \n",
    "\n",
    "            # device_type feature value counts, no of unique device_type, mean and std.    \n",
    "            device_type_count  = [0] * len(device_type_frequency)\n",
    "            for i,v in enumerate(group_set.device_type .values):\n",
    "                device_type_count[device_type_frequency[v]] += 1 \n",
    "            device_type_count.append(len(np.unique(group_set.device_type.values)))\n",
    "            _, device_type_unique = np.unique(group_set.device_type.values, return_counts=True)\n",
    "            device_type_count += [len(device_type_unique), np.mean(device_type_unique), np.std(device_type_unique)]        \n",
    "            features = features + device_type_count    \n",
    "            \n",
    "            # creating features from 'secs_elapsed' feature\n",
    "            secs_features = [0] * 5 \n",
    "            log_bin = [0] * 15\n",
    "            if len(secs) > 0:\n",
    "                secs_features[0] = np.log(1 + np.sum(secs))\n",
    "                secs_features[1] = np.log(1 + np.mean(secs)) \n",
    "                secs_features[2] = np.log(1 + np.std(secs))\n",
    "                secs_features[3] = np.log(1 + np.median(secs))\n",
    "                secs_features[4] = secs_features[0] / float(features[1])\n",
    "                \n",
    "                # bined features  \n",
    "                secs_log = np.log(1 + secs).astype(int)\n",
    "                log_bin = np.bincount(secs_log, minlength=15).tolist()                      \n",
    "            features = features + secs_features + log_bin\n",
    "            matrix.append(features)\n",
    "    \n",
    "    # creating feature names for matrix\n",
    "    feat_names = []    \n",
    "    for i in range(len(matrix[0])-1):\n",
    "        feat_names.append('feat_' + str(i)) \n",
    "    \n",
    "    # final feature matrix\n",
    "    matrix = np.array(matrix)\n",
    "    matrix_array = matrix[:, 1:].astype(np.float16)\n",
    "    matrix_id = matrix[:, 0]\n",
    "    \n",
    "     # creating dataframe from array matrix\n",
    "    session_matrix_dataframe = pd.DataFrame(matrix_array, columns=feat_names)\n",
    "    session_matrix_dataframe['id'] = matrix_id\n",
    "    session_matrix_dataframe.index = session_matrix_dataframe.id\n",
    "    \n",
    "    # Pre-processing test data\n",
    "    dataframe_tt = test_dataframe\n",
    "    dataframe_tt.index = dataframe_tt.id\n",
    "    dataframe_tt = dataframe_tt.fillna(-1)  \n",
    "    dataframe_tt = dataframe_tt.replace('-unknown-', -1) \n",
    "\n",
    "    # Feature extration from timestamp feature 'date_account_created'\n",
    "    dataframe_tt = dataframe_tt.drop(['date_first_booking'], axis=1)\n",
    "    dataframe_tt['n_null'] = np.array([sum(r == -1) for r in dataframe_tt.values])\n",
    "\n",
    "    date_acc_crt = np.vstack(dataframe_tt.date_account_created.astype(str).apply(lambda x: x.split('-')).values)\n",
    "    date_acc_crt=date_acc_crt.astype(int) \n",
    "    dataframe_tt['dac_year'] = date_acc_crt[:,0]\n",
    "    dataframe_tt['dac_month'] = date_acc_crt[:,1]\n",
    "    dataframe_tt['dac_day'] = date_acc_crt[:,2]\n",
    "\n",
    "    acc_cret_dates = [datetime(x[0],x[1],x[2]) for x in date_acc_crt]\n",
    "    dataframe_tt['dac_week_number'] = np.array([d.isocalendar()[1] for d in acc_cret_dates])\n",
    "    dataframe_tt['dac_week_day'] = np.array([d.weekday() for d in acc_cret_dates])\n",
    "\n",
    "    dataFrame_tt_wd = pd.get_dummies(dataframe_tt.dac_week_day, prefix='dac_week_day')\n",
    "    dataframe_tt = dataframe_tt.drop(['date_account_created', 'dac_week_day'], axis=1)\n",
    "    dataframe_tt = pd.concat((dataframe_tt, dataFrame_tt_wd), axis=1)\n",
    "    \n",
    "    # function\n",
    "    def func(s):\n",
    "        s=str(s)\n",
    "        return datetime(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8]),\n",
    "               hour=int(s[8:10]), minute=int(s[10:12]), second=int(s[12:]))\n",
    "    \n",
    "    # Feature extration from feature 'timestamp_first_active'\n",
    "    dataframe_tt['timestamp_first_active'] = pd.to_datetime(dataframe_tt.timestamp_first_active.apply(func))\n",
    "    first_acc_dates= list(dataframe_tt['timestamp_first_active'])\n",
    "\n",
    "    dataframe_tt['tfa_day'] = dataframe_tt.timestamp_first_active.dt.day\n",
    "    dataframe_tt['tfa_month'] = dataframe_tt.timestamp_first_active.dt.month\n",
    "    dataframe_tt['tfa_year'] = dataframe_tt.timestamp_first_active.dt.year\n",
    "    dataframe_tt['tfa_hour'] = dataframe_tt.timestamp_first_active.dt.hour\n",
    "    dataframe_tt['tfa_week_number'] = np.array([d.isocalendar()[1] for d in first_acc_dates])\n",
    "    dataframe_tt['tfa_week_day'] = np.array([d.weekday() for d in first_acc_dates])\n",
    "\n",
    "    dataFrame_tt_wd = pd.get_dummies(dataframe_tt.tfa_week_day, prefix='tfa_week_day')\n",
    "    dataframe_tt = dataframe_tt.drop(['timestamp_first_active', 'tfa_week_day'], axis=1)\n",
    "    dataframe_tt = pd.concat((dataframe_tt, dataFrame_tt_wd), axis=1)\n",
    "\n",
    "    # Extracting difference sign and difference in second between the time account_created and first_active\n",
    "    dataframe_tt['dac_tfa_secs'] = np.array([np.log(1+abs((acc_cret_dates[i]-first_acc_dates[i]).total_seconds())) for i in range(len(acc_cret_dates))])\n",
    "    dataframe_tt['sig_dac_tfa'] = np.array([np.sign((acc_cret_dates[i]-first_acc_dates[i]).total_seconds()) for i in range(len(acc_cret_dates))])\n",
    "    \n",
    "    # function indicator_season\n",
    "    def indicator_season(date_for_season):\n",
    "        date_for_season=date_for_season.date().replace(year=2000)\n",
    "\n",
    "        winter=[date(2000,  1,  1),date(2000,  3, 20),\n",
    "                date(2000, 12, 21),date(2000, 12, 31)]    \n",
    "        spring=[date(2000,  3, 21),  date(2000,  6, 20)]  \n",
    "        summer=[date(2000,  6, 21),  date(2000,  9, 22)]\n",
    "        autumn=[date(2000,  9, 23),  date(2000, 12, 20)]  \n",
    "\n",
    "        if (winter[0]<=date_for_season<=winter[1]) or (winter[2]<=date_for_season<=winter[3]):\n",
    "            sesn=0\n",
    "        elif spring[0]<=date_for_season<=spring[1]:\n",
    "            sesn=1\n",
    "        elif summer[0]<=date_for_season<=summer[1]:\n",
    "            sesn=2\n",
    "        elif autumn[0]<=date_for_season<=autumn[1]:\n",
    "            sesn=3\n",
    "        return sesn\n",
    "    \n",
    "    # Extracting Season feature from account_created and first_active dates\n",
    "    dataframe_tt['season_dac'] = np.array([indicator_season(date_for_season) for date_for_season in acc_cret_dates])\n",
    "    dataframe_tt['season_tfa'] = np.array([indicator_season(date_for_season) for date_for_season in first_acc_dates])\n",
    "    \n",
    "    # Pre-processing 'age' feature\n",
    "    age_value = dataframe_tt.age.values\n",
    "    age_value = np.where((age_value<2000)&(age_value>1900), 2014-age_value, age_value) \n",
    "    age_value = np.where((age_value<14)&(age_value>0), 4, age_value) \n",
    "    age_value = np.where((age_value<2016)&(age_value>2010), 9, age_value) \n",
    "    age_value = np.where(age_value>99, 110, age_value) \n",
    "    dataframe_tt['age'] = age_value\n",
    "    \n",
    "    # function indicator_season\n",
    "    age_interval =[i for i in range(0,101,5)]\n",
    "    def get_interv_value(age):\n",
    "        interval_value = 20\n",
    "        for i in range(len(age_interval)):\n",
    "            if age < age_interval[i]:\n",
    "                interval_value = i \n",
    "                break\n",
    "        return interval_value\n",
    "\n",
    "    dataframe_tt['age_interv'] = dataframe_tt.age.apply(lambda x: get_interv_value(x))\n",
    "    dataFrame_age_interval = pd.get_dummies(dataframe_tt.age_interv, prefix='age_interv')\n",
    "    dataframe_tt = dataframe_tt.drop(['age_interv'], axis=1)\n",
    "    dataframe_tt = pd.concat((dataframe_tt, dataFrame_age_interval), axis=1)\n",
    "    \n",
    "    # Creating dummy variables (one-hot-encoding) for train data features\n",
    "    one_hot_features = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "    for feature in one_hot_features:\n",
    "        dataFrame_tt_dummy = pd.get_dummies(dataframe_tt[feature], prefix=feature)\n",
    "        dataframe_tt = dataframe_tt.drop([feature], axis=1)\n",
    "        dataframe_tt = pd.concat((dataframe_tt, dataFrame_tt_dummy), axis=1)    \n",
    "\n",
    "    # Merging pre-processed session data and test data on 'id' to obtain final dataframe    \n",
    "    dataframe_tt.reset_index(drop=True,inplace=True)\n",
    "    session_matrix_dataframe.reset_index(drop=True,inplace=True)\n",
    "    final_dataFrame = pd.merge(dataframe_tt, session_matrix_dataframe, on=\"id\",how='left')\n",
    "    final_dataFrame = final_dataFrame.fillna(-2)\n",
    "    final_dataFrame['all_null'] = np.array([sum(r<0) for r in final_dataFrame.drop(['id'], axis=1).values])\n",
    "    \n",
    "    # feature selection\n",
    "    tr_cols=in_dict[\"features\"][\"fin_cols\"]\n",
    "    ts_cols=list(final_dataFrame.columns)\n",
    "\n",
    "    # format data for correct representation\n",
    "    test_formated=pd.DataFrame(data=np.zeros((s_id.shape[0],len(tr_cols))),columns=tr_cols)\n",
    "\n",
    "    for i in ts_cols:\n",
    "        test_formated[i]=final_dataFrame[i]\n",
    "     \n",
    "    # feature trasform and standarscalar\n",
    "    data = test_formated.values\n",
    "    ids_test=data[:,0:1]\n",
    "    data=data[:,1:]\n",
    "    data = data[:, in_dict[\"features\"][\"imp_features\"]]\n",
    "    X_test = in_dict[\"StandardScaler\"].transform(data)\n",
    "    \n",
    "    # load model and label encoder for selectioon\n",
    "    xgb=joblib.load(\"xgb_8000_best\")\n",
    "    lab_encoder=in_dict[\"lab_encoder\"]\n",
    "    \n",
    "    # final predictions\n",
    "    if prob_label==\"label\":\n",
    "        pred_targets=lab_encoder.inverse_transform(xgb.predict(X_test))\n",
    "    \n",
    "    elif prob_label==\"probality\":\n",
    "        pred_targets=xgb.predict_proba(X_test)\n",
    "        \n",
    "    \n",
    "    return pred_targets\n",
    "\n",
    "\n",
    "# FUNCTION-2 DEFINITION\n",
    "def FUNCTION2(prediction,Target_Labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------\n",
    "    Function computes Ndcg_Score for given Predictions and Target_labels\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prediction <ndarray>        : Contain predicted Probabilites\n",
    "        Target_Labels <pd.series>   : True lables\n",
    "    \n",
    "        returns \n",
    "        --------\n",
    "        Ndcg_score <float>          : Final Test score \n",
    "    -----------------------------------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    if len(prediction.shape)==1:\n",
    "        print(\"Error: FUNCTION2() needs Target probability for Score please Rerun FUNCTION1() with Output_choice as 2.\")\n",
    "        return 0.0\n",
    "    \n",
    "    # obtain ground_truth values\n",
    "    ground_truth=in_dict[\"lab_encoder\"].transform(Target_Labels)\n",
    "\n",
    "    # DCG Scorer function setup\n",
    "    def dcg_score(y_true, y_score, k=5):\n",
    "\n",
    "        # Compute releveace values for predictions\n",
    "        order = np.argsort(y_score)[::-1]\n",
    "        y_true = np.take(y_true, order[:k])\n",
    "\n",
    "        # compute DCG@k for a given point\n",
    "        dcg_numerator   = 2 ** y_true - 1\n",
    "        dcg_denominator = np.log2( np.arange( len(y_true) ) + 2 )\n",
    "        dcg_score = np.sum( dcg_numerator / dcg_denominator )\n",
    "\n",
    "        return dcg_score\n",
    "\n",
    "    # NDCG Scorer function setup\n",
    "    def ndcg_score(ground_truth, predictions, k=5):\n",
    "\n",
    "        # Compute relevance values for ground_truth\n",
    "        T =  LabelBinarizer().fit(range(predictions.shape[1] + 1)).transform(ground_truth)\n",
    "\n",
    "        # Compute NDCG@k score for all samples\n",
    "        scores = []\n",
    "        for y_true, y_score in zip(T, predictions):\n",
    "\n",
    "            dcg_k = dcg_score( y_true, y_score, k)\n",
    "            idcg_k = dcg_score( y_true, y_true, k)\n",
    "            ndcg_k = float(dcg_k) / float(idcg_k)\n",
    "            scores.append(ndcg_k)\n",
    "\n",
    "        # Mean of all scores\n",
    "        ndcg_score=np.mean(scores)\n",
    "\n",
    "        return ndcg_score\n",
    "    \n",
    "    # obtain final score\n",
    "    score=ndcg_score(ground_truth,prediction,5)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IT1CR0x7vK-6"
   },
   "source": [
    "# Run Below cell to Invoke FUNCTION-1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UR9jq21tvK-8",
    "outputId": "d335a3cf-1ea2-44b3-ea24-2d7172d4fd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number 1 or 2 based on your choice\n",
      "1.Enter Output choice: 1.Target labels  2.Target probability:>> 1\n",
      "---------------------- ---------------------- ---------------------- ---------------------- ---------------------- \n",
      "2.Enter Prediction choice:  1.Predict for single sample point  2.Predict for Random set of samples points:>> 2\n",
      "---------------------- ---------------------- ---------------------- ---------------------- ---------------------- \n",
      "3.Enter Data choice:  1.Predict using only train data  2.Predict using both train and sess_log data:>> 1\n",
      "---------------------- ---------------------- ---------------------- ---------------------- ---------------------- \n",
      "4.Select the size for Random set of samples points for Prediction: 50\n",
      "---------------------- ---------------------- ---------------------- ---------------------- ---------------------- \n",
      "----------------------------\n",
      "|  <<Selected Data Info>>  |\n",
      "----------------------------\n",
      "| Test_Data_size:  (50, 15) |\n",
      "| Target_Prediction_size: 50|\n",
      "----------------------------\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prediction Samples: ['NDF' 'US' 'NDF' 'NDF' 'US']\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO CALL FUNCTION-1()\n",
    "\n",
    "\"\"\"\n",
    "Obtaining Data samples by Random from Csv files based on given Data, Ouput and Prediction choices\n",
    "and also based on Number of Samples selected where obtained data is used as Test samples for predictions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Code to obtain various choices to perform required action\n",
    "print(\"Enter number 1 or 2 based on your choice\")\n",
    "output_choice=int(input(\"1.Enter Output choice: 1.Target labels  2.Target probability:>> \"))\n",
    "print(\"---------------------- \"*5)\n",
    "prediction_choice=int(input(\"2.Enter Prediction choice:  1.Predict for single sample point  2.Predict for Random set of samples points:>> \"))\n",
    "print(\"---------------------- \"*5)\n",
    "Data_choice=int(input(\"3.Enter Data choice:  1.Predict using only train data  2.Predict using both train and sess_log data:>> \"))\n",
    "print(\"---------------------- \"*5)\n",
    "\n",
    "if prediction_choice==1 and Data_choice==1:\n",
    "    \n",
    "    tr_data, sess_data= pd.read_csv('train_users.csv').sample(n=1),None\n",
    "\n",
    "elif prediction_choice==1 and Data_choice==2:\n",
    "    \n",
    "    while (True):\n",
    "        tr_data= pd.read_csv('train_users.csv').sample(n=1)\n",
    "        sess_data=sess_csv[sess_csv['user_id'].isin(list(tr_data[\"id\"].values))]\n",
    "        if(sess_data.shape[0]!=0):\n",
    "            break\n",
    "\n",
    "elif prediction_choice==2 and Data_choice==1:\n",
    "    \n",
    "    n=int(input(\"4.Select the size for Random set of samples points for Prediction: \"))\n",
    "    print(\"---------------------- \"*5)\n",
    "    tr_data, sess_data= pd.read_csv('train_users.csv').sample(n),None\n",
    "\n",
    "elif prediction_choice==2 and Data_choice==2:\n",
    "    \n",
    "    n=int(input(\"4.Select the size for Random set of samples points for Prediction: \"))\n",
    "    print(\"---------------------- \"*5)\n",
    "    \n",
    "    while (True):\n",
    "        tr_data= pd.read_csv('train_users.csv').sample(n)\n",
    "        sess_data=sess_csv[sess_csv['user_id'].isin(list(tr_data[\"id\"].values))]\n",
    "        \n",
    "        if(sess_data.shape[0]!=0):\n",
    "            break\n",
    "\n",
    "if 'country_destination' in tr_data.columns:\n",
    "    Target_Labels=tr_data['country_destination']\n",
    "    tr_data.drop(['country_destination'], axis=1,inplace=True)            \n",
    "\n",
    "\n",
    "    \n",
    "# printing Test_set info    \n",
    "print(\"--------------\"*2)    \n",
    "print(\"|  <<Selected Data Info>>  |\")\n",
    "print(\"--------------\"*2)\n",
    "print(\"| Test_Data_size:  {} |\".format(tr_data.shape))  \n",
    "if Data_choice!=1: print(\"| Sess_Data_size: {} |\".format(sess_data.shape))      \n",
    "prob_label= \"probality\" if output_choice==2 else \"label\"\n",
    "\n",
    "\n",
    "# Invoke FUNCTION-1()\n",
    "prediction=FUNCTION1([tr_data, sess_data], prob_label)\n",
    "\n",
    "print(\"| Target_Prediction_size: {}|\".format(len(prediction)))\n",
    "print(\"--------------\"*2)\n",
    "print(\"----------\"*11)\n",
    "\n",
    "\n",
    "# print Formated Final Predictions \n",
    "if (prob_label==\"label\"):\n",
    "    print(\"\\nPrediction Samples:\",prediction[0:5])\n",
    "\n",
    "else:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html\n",
    "    single_out=dict(zip(in_dict[\"lab_encoder\"].classes_, np.round(prediction[0],3)))\n",
    "    print(\"\\nPrediction Sample:\",single_out)\n",
    "    df=pd.DataFrame({\"Destinations\":in_dict[\"lab_encoder\"].classes_,\"Probabilites\":np.round(prediction[0],5)})\n",
    "    df.plot.bar(x=\"Destinations\", y=\"Probabilites\",title=\"Probabilistic Prediction for a point\",cmap=\"Dark2\",figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTEqWeA3vK_K"
   },
   "source": [
    "# Run Below cell to Invoke FUNCTION-2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvktzWXmvK_t",
    "outputId": "8210edd9-81ac-4f22-a0f1-06ce40bb425a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG_Score: 0.80349\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO CALL FUNCTION-2()\n",
    "NDCG_Score=FUNCTION2(prediction,Target_Labels)\n",
    "print(\"NDCG_Score:\",np.round(NDCG_Score,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6W2yWa8vK_4"
   },
   "source": [
    "## Run Below cells to see the Test Data that was selected randomly for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "804LsVV8vK_6",
    "outputId": "f2dd2229-e85c-4f41-9f61-b173ec028bef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_account_created</th>\n",
       "      <th>timestamp_first_active</th>\n",
       "      <th>date_first_booking</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>signup_method</th>\n",
       "      <th>signup_flow</th>\n",
       "      <th>language</th>\n",
       "      <th>affiliate_channel</th>\n",
       "      <th>affiliate_provider</th>\n",
       "      <th>first_affiliate_tracked</th>\n",
       "      <th>signup_app</th>\n",
       "      <th>first_device_type</th>\n",
       "      <th>first_browser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l2i8p7kf7m</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>20120128101544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sem-non-brand</td>\n",
       "      <td>google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>Other/Unknown</td>\n",
       "      <td>-unknown-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e9d4umazv6</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>20130604193141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>40.0</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v7ovrhq7ux</td>\n",
       "      <td>2014-03-10</td>\n",
       "      <td>20140310071224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>29.0</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5s28lmpgu1</td>\n",
       "      <td>2013-12-27</td>\n",
       "      <td>20131227164631</td>\n",
       "      <td>2014-03-05</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>42.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>sem-brand</td>\n",
       "      <td>google</td>\n",
       "      <td>linked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Safari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id date_account_created  timestamp_first_active date_first_booking  \\\n",
       "0  l2i8p7kf7m           2012-01-28          20120128101544                NaN   \n",
       "1  e9d4umazv6           2013-06-04          20130604193141                NaN   \n",
       "2  v7ovrhq7ux           2014-03-10          20140310071224                NaN   \n",
       "3  5s28lmpgu1           2013-12-27          20131227164631         2014-03-05   \n",
       "\n",
       "      gender   age signup_method  signup_flow language affiliate_channel  \\\n",
       "0  -unknown-   NaN         basic            1       en     sem-non-brand   \n",
       "1     FEMALE  40.0         basic            0       en            direct   \n",
       "2     FEMALE  29.0         basic            0       en            direct   \n",
       "3     FEMALE  42.0      facebook            0       en         sem-brand   \n",
       "\n",
       "  affiliate_provider first_affiliate_tracked signup_app first_device_type  \\\n",
       "0             google                     NaN        Web     Other/Unknown   \n",
       "1             direct               untracked        Web   Windows Desktop   \n",
       "2             direct               untracked        Web       Mac Desktop   \n",
       "3             google                  linked        Web       Mac Desktop   \n",
       "\n",
       "  first_browser  \n",
       "0     -unknown-  \n",
       "1        Chrome  \n",
       "2        Chrome  \n",
       "3        Safari  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.reset_index(drop=True,inplace=True) \n",
    "tr_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hlFFTdevLAR",
    "outputId": "7c8eb6d7-0795-4afc-8a72-b1d3308ab1ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date_account_created', 'timestamp_first_active',\n",
       "       'date_first_booking', 'gender', 'age', 'signup_method', 'signup_flow',\n",
       "       'language', 'affiliate_channel', 'affiliate_provider',\n",
       "       'first_affiliate_tracked', 'signup_app', 'first_device_type',\n",
       "       'first_browser'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noGIO3d1vLAm",
    "outputId": "257a69b2-cfe5-4f41-ce9a-d165ed92168f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>action</th>\n",
       "      <th>action_type</th>\n",
       "      <th>action_detail</th>\n",
       "      <th>device_type</th>\n",
       "      <th>secs_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174502</th>\n",
       "      <td>ldgtf9goal</td>\n",
       "      <td>dashboard</td>\n",
       "      <td>view</td>\n",
       "      <td>dashboard</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>232564.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174503</th>\n",
       "      <td>ldgtf9goal</td>\n",
       "      <td>authenticate</td>\n",
       "      <td>submit</td>\n",
       "      <td>login</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174504</th>\n",
       "      <td>ldgtf9goal</td>\n",
       "      <td>header_userpic</td>\n",
       "      <td>data</td>\n",
       "      <td>header_userpic</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174505</th>\n",
       "      <td>ldgtf9goal</td>\n",
       "      <td>personalize</td>\n",
       "      <td>data</td>\n",
       "      <td>wishlist_content_update</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id          action action_type            action_detail  \\\n",
       "174502  ldgtf9goal       dashboard        view                dashboard   \n",
       "174503  ldgtf9goal    authenticate      submit                    login   \n",
       "174504  ldgtf9goal  header_userpic        data           header_userpic   \n",
       "174505  ldgtf9goal     personalize        data  wishlist_content_update   \n",
       "\n",
       "        device_type  secs_elapsed  \n",
       "174502  Mac Desktop      232564.0  \n",
       "174503  Mac Desktop           NaN  \n",
       "174504  Mac Desktop         336.0  \n",
       "174505  Mac Desktop         138.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess_data.drop([\"id\"],inplace=True,axis=1)\n",
    "sess_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5Q2hD6vvLA6",
    "outputId": "c84c8e20-6b68-4149-f99d-026fad456477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'action', 'action_type', 'action_detail', 'device_type',\n",
       "       'secs_elapsed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
